{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 词向量\n",
    "word_vectors_cache = \"\"\n",
    "from torchnlp.word_to_vector import GloVe, FastText\n",
    "# GloVe: 参数name, 可取['6B', '42B', '840B', 'twitter.27B']; 参数cache, 设置词向量缓存地址\n",
    "\t# 若name为'6B'或'twitter.27B', 还有一个参数dim, 即确定获取的词向量维数\n",
    "word_vectors = GloVe(name='6B', dim=300, cache=word_vectors_cache) # 'GloVe_6B', 'GloVe_42B', 'GloVe_840B', 'GloVe_twitter.27B'\n",
    "# FastText: 参数为language='en', cache同上\n",
    "\t# 会出现问题: Calling FastText() results to HTTP Error 301: Moved Permanently\n",
    "\t# 可改用: from torchtext.vocab import FastText\n",
    "word_vectors = FastText(language='en', cache=word_vectors_cache) # 会出现问题: urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
    "\n",
    "# 编码\n",
    "from torchnlp.encoders.text import SpacyEncoder # 第一个参数为文本字符串列表 \n",
    "text_corpus = []\n",
    "encoder = SpacyEncoder(text_corpus, min_occurrences=3)\n",
    "encoder.encode() # 索引化, 得到字符串的编码整数序列 \n",
    "encoder.vocab # 得到词汇列表 \n",
    "encoder.decode() # 输入字符串编码序列得到对应的字符串 \n",
    "encoder.vocab_size # 词汇表大小\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_INDEX, DEFAULT_SOS_INDEX, DEFAULT_UNKNOWN_INDEX, DEFAULT_PADDING_INDEX\n",
    "print(DEFAULT_EOS_INDEX, DEFAULT_UNKNOWN_INDEX, DEFAULT_PADDING_INDEX, DEFAULT_SOS_INDEX) # 2 1 0 3 \n",
    "from torchnlp.encoders.text.text_encoder import stack_and_pad_tensors # 按一个batch中最长的进行pad, 不足的以pad值填充\n",
    "# 返回SequenceBatch: pad好的张量以及张量的原始长度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "\n",
    "from torchnlp.datasets.dataset import Dataset\n",
    "train_set = Dataset(examples=[{},...,{}])\n",
    "train_set.columns.add('index')\n",
    "\n",
    "# 迭代\n",
    "from torchnlp.utils import datasets_iterator # 用在循环中, 对数据进行迭代\n",
    "test_set = None\n",
    "text_corpus = [row['text'] for row in datasets_iterator(train_set, test_set)]\n",
    "# 一般更倾向于使用itertools.chain\n",
    "\n",
    "# 采样器\n",
    "from torchnlp.samplers import BucketBatchSampler\n",
    "dataset = None\n",
    "BucketBatchSampler(dataset, batch_size=32, drop_last=False, sort_key=lambda r: len(r['text'])) # 按长度从大到小排列, 然后以批大小重新组合数据"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
