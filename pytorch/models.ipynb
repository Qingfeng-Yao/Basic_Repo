{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变换\n",
    "\n",
    "nn.Embedding()\n",
    "\t# nn.Embedding(vocabulary_size, embedding_dimension).from_pretrained(vectors, freeze)\n",
    "\t# 默认freeze=True; 若为True, 则张量在学习过程中不会被更新. 等价于embedding.weight.requires_grad = False\n",
    "\t# .weight为可学习权重, 从标准正态分布初始化\n",
    "\t# embeddings.weight.data.copy_(vectors), 等价于embedding.weight.data[i] = vectors[token]\n",
    "nn.Linear()\n",
    "\t# 参数bias默认为True\n",
    "\t# .weight/.bias\n",
    "\t# .weight.data.shape; .weight.data.copy_()\n",
    "\t# nn.Linear(20, 30), .weight的shape为30,20\n",
    "\t# 不要使用太大的线性层, 因为nn.Linear(m,n)使用的是的O(mn)的内存, 线性层太大很容易超出现有显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积\n",
    "\n",
    "# torch.nn.Conv2d(channels, output, height_of_filter, width_of_filter) \n",
    "x = torch.randn(2,1,7,3) # 输入的形状[batch_size, channels, height_of_image, width_of_image]\n",
    "model = torch.nn.Conv2d(1,8,(2,3)) # height_of_filter, width_of_filter也可由一个值kernel_size给出; stride步长参数也可由一个元组或一个值给出\n",
    "res = model(x)\n",
    "print(res.shape) # torch.Size([2, 8, 6, 1]), 输出的高和宽分别是height_of_image-height_of_filter+1, width_of_image-width_of_filter+1\n",
    "\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "m = nn.Conv2d(16, 33, 3, stride=2) \n",
    "print(m(input).shape)  # torch.Size([20, 33, 24, 49]), 输出的高和宽需要除以stride再+1\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) # padding是在输入的两边添加0\n",
    "print(m(input).shape) # torch.Size([20, 33, 28, 100])\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) # dilation指核元素之间的Spacing, 默认为1, 如height_of_filter=(height_of_filter-1)*dilation+1\n",
    "print(m(input).shape) # torch.Size([20, 33, 26, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "\n",
    "# nn.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False)\n",
    "# 输入为input, (h_0, c_0); 其中input形状为(seq_len, batch, input_size), h_0, c_0形状均为(num_layers * num_directions, batch, hidden_size)\n",
    "# 输出为output, (h_n, c_n); 其中output形状为(seq_len, batch, num_directions * hidden_size), h_n, c_n形状同上\n",
    "# 不要在太长的序列上使用RNN; 因为RNN反向传播使用的是BPTT算法, 其需要的内存和输入序列的长度呈线性关系\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "print(output.shape, hn.shape, cn.shape) # torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])\n",
    "\n",
    "# nn.utils.rnn.pack_padded_sequence\n",
    "# x = pack_padded_sequence(x, x_len), 当数据集有长度为0的句子时, 会报错 \n",
    "# nn.utils.rnn.pad_packed_sequence\n",
    "seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])\n",
    "lens = [2, 1, 3]\n",
    "packed = nn.utils.rnn.pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False) \n",
    "# 压缩包含可变长度填充序列的张量\n",
    "# 若enforce_sorted为True, 则lens必须是降序\n",
    "'''\n",
    "PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\n",
    "batch_sizes的长度就是mini_batch数据中句子的最大长度, 每个元素代表列的长度\n",
    "sorted_indices中的索引为原始索引, 且排好序\n",
    "unsorted_indices的索引为结果索引, 对应原始的顺序\n",
    "'''\n",
    "seq_unpacked, lens_unpacked = nn.utils.rnn.pad_packed_sequence(packed, batch_first=True) # 还有参数padding_value=0.0, total_length=None\n",
    "# 填充一批包装好的可变长度序列\n",
    "'''\n",
    "(tensor([[1, 2, 0],\n",
    "         [3, 0, 0],\n",
    "         [4, 5, 6]]), tensor([2, 1, 3]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "\n",
    "# nn.TransformerEncoderLayer这个类是transformer encoder的组成部分, 代表encoder的一个层\n",
    "# 而encoder就是将transformerEncoderLayer重复几层\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) \n",
    "# d_model: 输入的特征维度\n",
    "# 还有参数dim_feedforward(默认2048), dropout, activation(默认relu)\n",
    "# 输入形状为(seqlenth x batch x dim)形式的数据\n",
    "\n",
    "# nn.TransformerEncoder是transformer的encoder部分, 即将上述的encoder-layer作为参数输入初始化以后可以获得TransformerEncoder\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "# num_layers指encoder-layer的数目; norm: 层normalization\n",
    "# 输入形状为(seqlenth x batch x dim)形式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化\n",
    "\n",
    "# 提取重要信息的操作, 可以去掉不重要的信息，减少计算开销\n",
    "nn.MaxPool2d() \n",
    "nn.AvgPool2d()\n",
    "# 参数情况可类比nn.Conv2d\n",
    "# 步长的默认值是kernel_size\n",
    "\n",
    "F.avg_pool2d() # 数据是四维输入\n",
    "\t# input形状: (batch_size, channels, height, width)\n",
    "F.max_pool1d() \n",
    "F.max_pool2d()\n",
    "F.lp_pool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "\n",
    "nn.Softmax()\n",
    "\t# F.softmax(input, dim=1)  # 按行softmax, 行和为1\n",
    "nn.LogSoftmax() # 对softmax的结果进行log, 即Log(Softmax(x))\n",
    "\n",
    "nn.ReLU(inplace=True) # 修正线性单元(Rectified Linear Unit, 整流线性单位函数), 通常指代以斜坡函数f(x)=max(0,x)及其变种为代表的非线性函数\n",
    "\t# torch.relu()\n",
    "\t# F.relu()\n",
    "\t# F.softplus() # relu函数的平滑版本\n",
    "nn.LeakyReLU(0.1)\n",
    "nn.ELU()\n",
    "nn.GELU()\n",
    "\n",
    "nn.Sigmoid()\n",
    "\t# 逻辑函数(Logistic sigmoid)\n",
    "\t# torch.sigmoid() # 将变量映射到0,1之间, 不使用nn.functional.sigmoid()\n",
    "\t# F.sigmoid()\n",
    "nn.Tanh() # 双曲函数\n",
    "\t# torch.tanh() # 图像为过原点并将变量映射到-1,1之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络相关\n",
    "\n",
    "nn.Module\n",
    "# 除__init__函数, 还要实现forward函数(从输入到输出)\n",
    "# module.weight.data, module.bias.data\n",
    "# register_buffer(name: str, tensor: torch.Tensor, persistent: bool = True) -> None: 为module添加一个buffer\n",
    "# register_parameter(name: str, param: torch.nn.parameter.Parameter) -> None: 为module添加一个parameter\n",
    "nn.ModuleList()\n",
    "nn.Sequential()\n",
    "\t# .add_module()\n",
    "\t# nn.Sequential(*layers) # layers为列表\n",
    "\t# model = nn.Sequential(nn.Linear(2, 2, bias=False))或model = nn.Sequential(nn.Linear(2, 2, bias=False), nn.Tanh()); 可使用model[0]访问线性网络\n",
    "\t# model.modules()\n",
    "\t# model的一些属性: training/bool, _parameters/OrderedDict(), _buffers/OrderedDict(), _modules/OrderedDict()\n",
    "# 可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数(包括通过继承得到的父类中的参数)\n",
    "params = list(model.named_parameters())\n",
    "(name, param) = params[28]\n",
    "print(name)\n",
    "print(param.grad)\n",
    "(name2, param2) = params[29]\n",
    "print(name2)\n",
    "print(param2.grad)\n",
    "(name1, param1) = params[30]\n",
    "print(name1)\n",
    "print(param1.grad)\n",
    "\n",
    "# 定义可训练的参数\n",
    "nn.Parameter() # Variable的一种, 参数为data\n",
    "# nn.Parameter(tensor, requires_grad)\n",
    "# 与torch.tensor(list, requires_grad=True)的区别: 这个只是将参数变成可训练的, 并没有绑定在module的parameter列表中\n",
    "\n",
    "nn.BatchNorm2d() # 参数是num_features, 默认是有可学习的参数, 若affine=False则无\n",
    "nn.BatchNorm1d()\n",
    "nn.LayerNorm(2, elementwise_affine=True)\n",
    "nn.utils.clip_grad_norm_() # 梯度裁剪(gradient clipping), 第一个参数一般为model.parameters(), 第二个参数为max_norm\n",
    "# 可以预防RNNs/LSTMs中的梯度爆炸问题\n",
    "F.normalize() # L2 normalization, 第一个参数tensor, 第二个参数dim\n",
    "\n",
    "dropout = 0\n",
    "nn.Dropout(dropout)\n",
    "\n",
    "pad = None\n",
    "F.pad(input, pad, mode='constant', value=0) # 便于对数据集图像或中间层特征进行维度扩充\n",
    "# pad: 扩充维度, 用于预先定义出某维度上的扩充参数\n",
    "\t# p1d = (1, 2)即(左边填充数, 右边填充数), 针对最后一维\n",
    "\t# p2d = (1, 2, 3, 4)即(左边填充数, 右边填充数, 上边填充数, 下边填充数), 针对最后两维\n",
    "# mode: 扩充方法, 'constant', 'reflect'或'replicate'三种模式, 分别表示常量, 反射, 复制\n",
    "# value: 扩充时指定补充值, 但是value只在mode='constant'有效, 即使用value填充在扩充出的新维度位置; 而在'reflect'和'replicate'模式下, value不可赋值\n",
    "\n",
    "nn.CosineSimilarity(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数/criterion\n",
    "\n",
    "nn.MSELoss()\n",
    "\n",
    "nn.BCELoss()\n",
    "nn.BCEWithLogitsLoss() # BCEWithLogitsLoss是针对二分类的CrossEntropy\n",
    "\n",
    "weight, reduction = None, None\n",
    "nn.CrossEntropyLoss(weight, reduction='mean') # 输入不需要经过Softmax; 参数weight: 为每个类指定权重, reduction: 对输出应用, 'none', 'mean', 'sum', 默认为mean\n",
    "# 等价于torch.nn.functional.log_softmax + torch.nn.NLLLoss\n",
    "input, target, weight = None, None, None\n",
    "F.cross_entropy(input, target, reduction='none', weight=weight) # weight确定每个类别的权重\n",
    "# 该函数使用了log_softmax和nll_loss\n",
    "F.nll_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "\n",
    "nn.init.constant_()\n",
    "nn.init.uniform_()\n",
    "nn.init.normal_()\n",
    "nn.init.xavier_uniform_()\n",
    "nn.init.xavier_normal_()\n",
    "nn.init.kaiming_uniform_()\n",
    "tensor = None\n",
    "nn.init.orthogonal_(tensor, gain=1) # 使得tensor是正交的"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
