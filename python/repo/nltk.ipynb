{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "\n",
    "directory = \"\"\n",
    "nltk.download('reuters', download_dir=directory) # 若数据集已经下载好, 即可注释掉这条语句\n",
    "if directory not in nltk.data.path:\n",
    "    nltk.data.path.append(directory)\n",
    "from nltk.corpus import reuters\n",
    "doc_ids = reuters.fileids()\n",
    "split_set = 'train' # 'test'\n",
    "split_set_doc_ids = list(filter(lambda doc: doc.startswith(split_set), doc_ids))\n",
    "# reuters.raw(id): 根据id获得原始文本\n",
    "# labels = reuters.categories(id): 根据id获得标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "\n",
    "# 分词\n",
    "from nltk import word_tokenize # 对英文文本进行分词, from nltk.tokenize import word_tokenize\n",
    "text = \"\"\n",
    "word_tokens = word_tokenize(text) # 得到一个列表\n",
    "\n",
    "# 停用词\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stem\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\") # 选择语言\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl = WordNetLemmatizer()\n",
    "# SnowballStemmer较为激进, 转换有可能出现错误\n",
    "# 这里较为推荐使用WordNetLemmatizer, 它一般只在非常肯定的情况下才进行转换, 否则会返回原来的单词\n",
    "print(stemmer.stem('knives'))\n",
    "# knive\n",
    "print(wnl.lemmatize('knives'))\n",
    "# knife"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
